{\rtf1\ansi\ansicpg1252\cocoartf949\cocoasubrtf540
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\vieww9000\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\ql\qnatural\pardirnatural

\f0\fs24 \cf0 We now turn to the problem of building a syntax and extend our acceptor into a parser.  As we did until now, we will keep this extension minimal.  Since we did not provide a way to distinguish operators or words among themselves, we will not keep any information on those.  All the information that we will have will be the shape of the syntax tree.  We will be able to add attributes to the tree when we need the information but we will delay it until then.\
\
Here is the definition of our syntax tree.\
\
tree ^= 	btree (l, r: tree)\
	|	leaf\
\
We will make explicit the assumptions that we will use on that structure:\
\
(0)	btree.x.y = btree.w.z    ==    x = w  /\\  y = z\
(1)	btree.x.y \uc0\u8800  leaf\
\
We assume that the tree admits well-founded order:\
\
(2)	x < btree.x.y\
(3)	y < btree.x.y\
(4)	z < x  ==>  z < btree.x.y\
(5)	z < y  ==>  z < btree.x.y\
(6)	( 
\b A
\b0  x:  ( 
\b A
\b0  y:  y < x:  P.y ):  P.x )  ==>  ( 
\b A
\b0  x::  P.x )\
\
Finally, we define the linearization of trees:\
\
(7)	lin.leaf = [ word ]\
(8)	lin.( btree.x.y ) = lin.x ++ [ op ] ++ lin.y\
\
A simple approach to building a tree would be to keep the tree built so far in a variable t and to build a new node every time a word is scanned, as long as the input sting is accepted and assign t := btree.t.leaf.  It has the nice (?) property that it is a deterministic behavior but the class of trees it builds is largely uninteresting.  Instead, we will build a nondeterministic tree building that can build any appropriate syntax tree for a given input.  We defer to later the task of removing the nondeterminacy by introducing an appropriate criterion for choosing the tree that interests us among those that interest us.  At that point, for the ease of use of the language, it will be important that the tree be uniquely associated with the input string and that the choice be based on a very simple criterion.  An acceptable trade-off might be to reject input that the acceptor accepted in order to make the selection simple.\
\
For now, let's say that we will build a stack def of trees for which the aggregation has been deferred.  So we have two actions to perform on def: integrate new words as leaves and aggregate trees.  \
\
In order to relate the def to the input we will introduce a serialization function and use it to formulate invariants.  \
\
It will be defined as the operator separated concatenation of the linearization of the trees def contains.  For simplicity, we add an operator at the end.  It will provide a valid serialization for the cases where L2.x as they end by an operator and to bridge the gap between this notion of serialization and words such that L1.x, we will add a tree variable t, for which we add the linearization after the serialization of def.\
\
(9)	ser.[] = []\
(10)	ser.( def ++ [ t ] ) = ser.def ++ lin.t ++ [ op ]\
\
	J1:	c  ==>  ser.def ++ lin.t = x\
	J2:	d  ==>  ser.def = x\
\
(11)		J1'\
	=		\{ P1: c' = d and P2: x' = x ++ [ y ] \}\
		d  ==>  ser.def' ++ lin.t' = x ++ [ y ]\
	=		\{ G1: y = word \}\
		d  ==>  ser.def' ++ lin.t' = x ++ [ word ]\
	=		\{ P3: def' = def and P4: t' = leaf \}\
		d  ==>  ser.def ++ lin.leaf = x ++ [ word ]\
	=		\{ (7) \}\
		J2\
\
(12)		J2'\
	=		\{ P5: d' = c and P2 \}\
		c  ==>  ser.def' = x ++ [ y ]\
	=		\{ G2: y = op \}\
		c  ==>  ser.def' = x ++ [ op ]\
	=		\{ P6: def' = def ++ [ t ] \}\
		c  ==>  ser.( def ++ [ t ] ) = x ++ [ op ]\
	=		\{ (10) \}\
		c  ==>  ser.def ++ lin.t ++ [ op ] = x ++ [ op ]\
	=		\{ Sequence theory \}\
		J1\
\
We used respectively y = word and y = op as hypotheses in the previous calculations.  What if we had swapped them?  In the conditional of the acceptor, when y = op, c gets assigned false and when y = word, d gets assigned false so this falsify the premises respectively of J1 and J2.  They are, therefore, trivially enforced them.\
\
Merging this with what we already had for the acceptor of infix expressions, we get:\
\
	if y = word ->\
		c, d, t := d, false, leaf\
	[] y = op ->\
		c, d, def := false, c, def ++ [ t ]\
	fi ||\
	x, z := x ++ [ y ], w\
\
If we want to end up with one tree for the whole input string, we need to merge trees from time to time.  Since it affects def and we already specified its final value for both branches, we could append or prepend our reduction to the program that we already have.  This could become complicated, especially around the termination because, as it stands, the loop terminates at the moment where z is empty.  Instead, we add another alternative that is to join two subtrees.  While one of the two other alternatives is activated whenever z is not empty, joining trees does not need that kind of assumption and we can weaken the guard of the loop, and strengthening its postcondition.  To merge subtrees, we need at least two trees in the stack, including t.  On the other hand, we need at most one tree upon termination which mean that def = [] will be sufficient.  \
\
Since we have t to stand for the top tree, we only need to remove one more tree from def to reduce it into a binary node.  We can make a first guess for a statement and a guard that would decrease def.\
\
	P7:	def' = s\
	P8:	t' = btree.u.t\
\
	G3:	def = s ++ [ u ]\
\
Since we change only def and t, we have no hope of transforming J1 into J2.  But this simplifies our calculations for the preservation of both invariants.  For J1, we will transform ser.def' ++ lin.t' into its unprimed version using only equality preserving transformations\
\
(13)		ser.def' ++ lin.t'\
	=		\{ P7 and P8 \}\
		ser.s ++ lin.( btree.u.t )\
	=		\{ (8) to switch u out of the lin term and\
				put it in the ser tern \}\
		ser.s ++ lin.u ++ [ op ] ++ lin.t\
	=		\{ (10) \}\
		ser.( s ++ [ u ] ) ++ lin.t\
	=		\{ G3 \}\
		ser.def ++ lin.t\
\
Adding a skip operation over c and x takes care of the rest of the invariant J1.  As for maintaining J2, we could use an analogous strategy but we see that x is unchanged and def is becoming smaller.  Adding a guard to that branch might as long as we don't prevent it to occur when necessary.  In other words, it should not weaken the postcondition.  \
\
	G4:	d == false\
\
Accumulating the guards of the loop will allow us to get to an exit condition.  The accumulated guard is:\
\
	G5:	( d == false /\\ ( 
\b E
\b0  s, u::  def = s ++ [ u ] ) ) \\/ ( 
\b E 
\b0 y, w::  [ y ] ++ w = z )\
\
Which gives us the following exit condition:\
\
	G6:	( d \uc0\u8800 \u8800  false \\/ \'ac ( 
\b E
\b0  s, u::  def = s ++ [ u ] ) ) /\\  ( 
\b E 
\b0 y, w::  [ y ] ++ w = z )\
\
		( d \uc0\u8800 \u8800  false \\/ "\'ac ( 
\b E
\b0  s, u::  def = s ++ [ u ] )" ) /\\  "\'ac ( 
\b E 
\b0 y, w::  [ y ] ++ w = z )"\
 	=		\{ Sequence theory \}\
		( d \uc0\u8800 \u8800  false \\/ def = [] ) /\\ z = []\
	=		\{ JA: \'ac c \\/ \'ac d \}\
		( \'ac c \\/ def = [] ) /\\ z = []\
	==>		\{ J1 \}\
		( c  ==>  lin.t = x ) /\\ z = []\
	==>		\{ J5 \}\
		c  ==>  lin.t = S\
\
This is exactly the postcondition we are interested in.  To reach it, we introduced invariant JA which is clearly established by the initialization.  In addition, whenever c or d are assigned to, one of them is assigned false which also takes care of it.\
\
In short, the effect new branch can be summarized by:\
\
	P9:	P7 /\\ P8\
\
From the previous developments, we already have the following:\
\
	P10:	c' = 
\b false
\b0 \
	P11:	d' = 
\b false
\b0 \
	P12:	x' = x ++ [ y ]\
	P13:	z' = w\
	GE:	z = [ y ] ++ w\
\
And, in summary, the old part of the body of the loop becomes:\
\
	P14:	( (G1 /\\ P10 /\\ P1 /\\ P3) \\/ (G2 /\\ P11 /\\ P5 /\\ P6) ) /\\ P12 /\\ P13\
\
Since P9 does not mention any other variables than def, that we just introduced, we know that it does not violate any of the old invariants.  To make sure that they are preserved, we can add a skip on all the old variables to P9.\
\
	P15:	c' = c  /\\  d' = d  /\\  x' = x  /\\  z' = z\
\
The body of our loop is now:\
\
	P16:	( P9 /\\ P15 ) \\/ P14\
\

\b Intermezzo
\b0 \
So far, we have used a technique based on invariants in a calculus where iteration is defined in terms of recursion.  This means we proceeded in an order which is not exactly top down.  Instead of writing the specification of the algorithm and refining it, eventually using recursion for the loop, we identified a task, close to our goal, that would be nicely accomplished by a loop and proceeded to build an invariant for it, only worrying about its place in respect to the specification once the deed was done.  However, the risk was small because we kept the specification in mind while building the loop to make sure that a constant effort, i.e. one which does not involve a loop, would be sufficient to bridge the gap. \
\
So far, we hadn't defended that it was sound to proceed that way and we will now do so.  We will defend the whole scheme rather than this particular instance and will therefore work with a loop preserving invariant J with body S1 and guard G.  In Hehner's book, the corresponding specification is noted:\
\
	S2:	J   ==>   \'ac G'  /\\  J'\
\
As mentioned, this is usually refined using recursion guarded by G and we will prove that our proof obligations are sufficient to get such a refinement.  The usual proof obligation that guided our steps so far looks like this:\
\
		J /\\ G /\\ S1   ==>   J'\
\
We will reformulate it to suit our manipulative need.  We use the shunting rule for that.\
\
	(14)	J   ==>   ( G /\\ S1   ==>   J' )\
\
Finally, the recursive form of the loop will be:\
\
	S3:	J   ==>   ( G /\\ ( S1 ; S2 ) ) \\/ ( \'ac G /\\ ok )\
\
We now want to prove that S3 refines S2:\
\
	(15)	S2   <==   S3\
\
We will assume J and transform the consequent of S3 into that of S2.  We start with the recursive branch of the conditional.\
\
		G /\\ ( S1 ; S2 )\
	  =		 \{ Definition of S2 \} \
		G /\\ ( S1 ; J  ==>  \'ac G' /\\ J' )\
	  =		\{ (16) see below \}\
		( G /\\ S1 ) ; ( J   ==>   \'ac G' /\\ J' )\
	  =		\{ (14) assuming J \}\
		( G /\\ S1 /\\ J' ) ; ( J   ==>   \'ac G' /\\ J' )\
	==>		\{ (17) and modus ponens \}\
		( G /\\ S1 ) ; ( \'ac G' /\\ J' )\
	==>		\{ (18) \}\
		\'ac G' /\\ J'\
\
Let's continue with the second branch:\
\
		\'ac G /\\ ok\
	  =		\{ assume J \}\
		\'ac G /\\ J /\\ ok\
	  =		\{ properties of ok \}\
		\'ac G' /\\ J' /\\ ok\
	==>		\{ Weakening \}\
		\'ac G' /\\ J'\
\
It is now easy to conclude the refinement.  In the proof of the first branch of the alternative, we used the following three properties.  We will only prove (18) since the other proofs are very similar and simpler.\
\
	(16)	A /\\ ( B ; C )   ==   ( A /\\ B ) ; C \
	(17)	( A /\\ B' ) ; C   ==   A ; ( B /\\ C )\
	(18)	B'  <==  A ; B' \
\
		A ; B'\
	=		\{ Definition of ; \}\
		( 
\b E
\b0  v''::  A [v' := v''] /\\ B' [v := v''] )\
	=		\{ There remains no unprimed copies of v in B'. \}\
		( 
\b E 
\b0 v''::  A [v' := v''] /\\ B' )\
	=		\{ /\\ distributes over 
\b E
\b0  \}\
		( 
\b E
\b0  v''::  A [v' := v''] ) /\\ B'\
	==>		\{ Weakening \}\
		B'\
\

\b end of intermezzo
\b0 \
\
In the interest of expressing a compact theorem of our findings so far, we will state what specification our loop satisfies in addition to termination.\
\
	J3:	c == L1.x\
	J4: 	d == L2.x\
	J5:	x ++ z = S\
\
	J6:	J1 /\\ J2 /\\ J3 /\\ J4 /\\ J5\
\
Using the theorem proved in the intermezzo, we assert that the following specification:\
\
	P17:	J6   ==>   \'ac G5' /\\ J6'\
\
is refined by the following recursive formulation of our loop:\
\
	P18:	J6   ==>    ( G5 /\\ ( P16 ; P17 ) ) \\/ ( \'ac G5 /\\ ok )\
\
And now for the statement of the refinement which is a special case of (15).\
\
	(19)	P17   <==   P18\
\
When it comes to termination, we can see that the two main branches have their own variants: z and def.  However, while the second branch does not modify z, the first one makes def grow.  We can't just add the size of the two sequences to obtain a variant global.  However, we can consider that (z, def) will be decreased as a tuple if we consider its lexicographical ordering.  We already know that the first branch decreases z, we don't need anything further in that respect.  We'll have to make sure that the second branch does not increase z  --as a matter of fact, it should not need to touch it--  and decreases def.  In order to prove that it actually decreases, we will first define the well founded order and then carry the proof for each branch.\
\
(20)	(z, def) < (z', def')    ==    z < z'  \\/  ( z = z' /\\ def < def' )\
\
Here, we use the principle that a lexicographical order on tuples whose elements admit a well-founded order is itself well-founded.\
\
(21)		(z', def') < (z, def)\
	=		\{ (20) \}\
		z' < z  \\/  ( z' = z  /\\  def' < def )\
	<==		\{ Strengthening \}\
		z' = z  /\\  def' < def\
	=		\{ z' = z \}\
		def' < def\
	=		\{ P7 and G3 \}\
		s   <   s ++ [ u ]\
	=		\{ Sequence theory \}\
		
\b true
\b0 \
\
(22)		(z', def') < (z, def)\
	<==		\{ (20) and strengthening \}\
		z' < z\
	=		\{ P13 \}\
		w < z\
	=		\{ GE \}\
		w   <   [ y ] ++ w\
	=		\{ Sequence theory \}\
		
\b true
\b0 \
\
Now, we know that our nondeterministic loop will terminate and produce a syntax tree if the input is correct.  It looks like this:\
\
	c, d, x, z, def := 
\b false
\b0 , 
\b true
\b0 , [], [], S\
	
\b while
\b0  z \uc0\u8800  []   \\/   #def \u8805  2 
\b do
\b0 \
		
\b if
\b0  
\b any
\b0  y, w 
\b where
\b0  y -> w = z 
\b in
\b0 \
			
\b if
\b0  y = word -->\
				c := d  ||\
				d := 
\b false
\b0   ||\
				t := leaf\
			[] y = op ->\
				c := 
\b false 
\b0  ||\
				d := c  ||\
				def := def ++ [ t ]\
			
\b fi  
\b0 ||\
			x := x ++ [ y ]  ||\
			z := w\
		[] 
\b any
\b0  s, u, v 
\b where
\b0  s <- u <- v = def 
\b in
\b0 \
			del := s <- btree.u.v\
		
\b fi
\b0 \
	
\b od
\b0 \
\
}